{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion as of Nov 16 2024\n",
    "- Squared deviation score rewards \"further from 50%\" strategies more\n",
    "- All scoring methods rank \"spread\" strategies the same\n",
    "- Brier score and squared deviation have the same average rank position changes from baseline score\n",
    "- Squared deviation scoring matches the brier score rankings better than baseline rankings\n",
    "- Squared deviation score is the best deviation score available thus far \n",
    "\n",
    "# Update as of Nov 17 2024\n",
    "- Expected log score is best analogy to baseline score when assuming community prediction is the true probability and the actual resolution is not yet known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Literal\n",
    "\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, true_probability: float) -> None:\n",
    "        assert 0.01 <= true_probability <= 0.99\n",
    "        self.true_probability: float = true_probability\n",
    "        self.resolution: bool = True if rng.random() < self.true_probability else False\n",
    "        self.community_prediction: float = self.true_probability\n",
    "\n",
    "\n",
    "class ScoringMethod(BaseModel):\n",
    "    score_function: Callable[[Question, float], float]\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.score_function.__name__\n",
    "\n",
    "\n",
    "class Forecaster(BaseModel):\n",
    "    prediction_strategy: Callable[[Question], float]\n",
    "    name: str\n",
    "\n",
    "    def predict(self, question: Question) -> float:\n",
    "        return self.prediction_strategy(question)\n",
    "\n",
    "\n",
    "class ForecasterPredictions:\n",
    "    def __init__(self, forecaster: Forecaster, questions: list[Question]) -> None:\n",
    "        self.forecaster = forecaster\n",
    "        self.questions = questions\n",
    "        self.predictions = [forecaster.predict(question) for question in questions]\n",
    "\n",
    "    def get_average_score(self, scoring_method: ScoringMethod) -> float:\n",
    "        scores = [\n",
    "            scoring_method.score_function(q, p)\n",
    "            for q, p in zip(self.questions, self.predictions)\n",
    "        ]\n",
    "        return float(np.average(scores))\n",
    "\n",
    "\n",
    "class Ranking:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecaster_predictions: list[ForecasterPredictions],\n",
    "        scoring_method: ScoringMethod,\n",
    "    ) -> None:\n",
    "        self.scoring_method = scoring_method\n",
    "        self.ranked_forecaster_predictions = sorted(\n",
    "            forecaster_predictions,\n",
    "            key=lambda x: x.get_average_score(self.scoring_method),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def ranked_forecasters(self) -> list[Forecaster]:\n",
    "        return [fp.forecaster for fp in self.ranked_forecaster_predictions]\n",
    "\n",
    "    @property\n",
    "    def ranked_scores(self) -> list[float]:\n",
    "        return [\n",
    "            fp.get_average_score(self.scoring_method)\n",
    "            for fp in self.ranked_forecaster_predictions\n",
    "        ]\n",
    "\n",
    "\n",
    "class Tournament:\n",
    "    def __init__(\n",
    "        self,\n",
    "        questions: list[Question],\n",
    "        forecasters: list[Forecaster],\n",
    "        scoring_methods: list[ScoringMethod],\n",
    "    ) -> None:\n",
    "        forecaster_names = [f.name for f in forecasters]\n",
    "        assert len(forecaster_names) == len(\n",
    "            set(forecaster_names)\n",
    "        ), \"All forecasters must have unique names\"\n",
    "\n",
    "        self.questions = questions\n",
    "        self.forecasters = forecasters\n",
    "        self.scoring_methods = scoring_methods\n",
    "        self.rankings = self.__get_rankings()\n",
    "\n",
    "    def __get_rankings(self) -> list[Ranking]:\n",
    "        forecaster_predictions = [\n",
    "            ForecasterPredictions(forecaster, self.questions)\n",
    "            for forecaster in self.forecasters\n",
    "        ]\n",
    "        rankings = [\n",
    "            Ranking(forecaster_predictions, scoring_method)\n",
    "            for scoring_method in self.scoring_methods\n",
    "        ]\n",
    "        return rankings\n",
    "\n",
    "\n",
    "class QuestionManager:\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_questions_sets(cls, num_questions_per_set: int) -> list[list[Question]]:\n",
    "        return [\n",
    "            cls.get_random_questions(num_questions_per_set),\n",
    "            cls.get_uncertain_questions(num_questions_per_set),\n",
    "            cls.get_certain_questions(num_questions_per_set),\n",
    "            cls.get_low_probability_questions(num_questions_per_set),\n",
    "            cls.get_high_probability_questions(num_questions_per_set),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_random_questions(cls, num_questions: int) -> list[Question]:\n",
    "        return [\n",
    "            Question(true_probability=rng.uniform(0.01, 0.99))\n",
    "            for _ in range(num_questions)\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_uncertain_questions(cls, num_questions: int) -> list[Question]:\n",
    "        return [\n",
    "            Question(true_probability=rng.uniform(0.1, 0.9))\n",
    "            for _ in range(num_questions)\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_certain_questions(cls, num_questions: int) -> list[Question]:\n",
    "        return [\n",
    "            Question(\n",
    "                true_probability=(\n",
    "                    rng.uniform(0.9, 0.99)\n",
    "                    if rng.random() < 0.5\n",
    "                    else rng.uniform(0.01, 0.1)\n",
    "                )\n",
    "            )\n",
    "            for _ in range(num_questions)\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_low_probability_questions(cls, num_questions: int) -> list[Question]:\n",
    "        return [\n",
    "            Question(true_probability=rng.uniform(0.01, 0.5))\n",
    "            for _ in range(num_questions)\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_high_probability_questions(cls, num_questions: int) -> list[Question]:\n",
    "        return [\n",
    "            Question(true_probability=rng.uniform(0.5, 0.99))\n",
    "            for _ in range(num_questions)\n",
    "        ]\n",
    "\n",
    "\n",
    "class ScoringMethodCreator:\n",
    "    @classmethod\n",
    "    def get_all_scoring_methods(cls) -> list[ScoringMethod]:\n",
    "        return [\n",
    "            cls.brier_score(),\n",
    "            cls.baseline_score(),\n",
    "            cls.squared_deviation_score(),\n",
    "            cls.exact_deviation_score(),\n",
    "            cls.log_deviation_score(),\n",
    "            cls.relative_entropy_score(),\n",
    "            cls.ratio_score(),\n",
    "            cls.geometric_mean_score(),\n",
    "            cls.log_odds_ratio_score(),\n",
    "            cls.squared_log_odds_ratio_score(),\n",
    "            cls.expected_brier_score(),\n",
    "            cls.expected_log_score(),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def baseline_score(cls) -> ScoringMethod:\n",
    "        def baseline_scoring(question: Question, prediction: float) -> float:\n",
    "            actual = 1.0 if question.resolution else 0.0\n",
    "            outcome_prob = prediction if abs(actual - 1.0) < 1e-4 else (1 - prediction)\n",
    "            return (np.log2(outcome_prob) + 1) * 100\n",
    "\n",
    "        return ScoringMethod(score_function=baseline_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def brier_score(cls) -> ScoringMethod:\n",
    "        def brier_scoring(question: Question, prediction: float) -> float:\n",
    "            actual = 1.0 if question.resolution else 0.0\n",
    "            return -((actual - prediction) ** 2)\n",
    "\n",
    "        return ScoringMethod(score_function=brier_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def log_deviation_score(cls) -> ScoringMethod:\n",
    "        def log_deviation_scoring(question: Question, prediction: float) -> float:\n",
    "            return abs(np.log2(prediction) - np.log2(question.community_prediction)) * -1\n",
    "\n",
    "        return ScoringMethod(score_function=log_deviation_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def squared_deviation_score(cls) -> ScoringMethod:\n",
    "        def squared_deviation_scoring(question: Question, prediction: float) -> float:\n",
    "            return abs(prediction - question.community_prediction) ** 2 * -1\n",
    "\n",
    "        return ScoringMethod(score_function=squared_deviation_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def exact_deviation_score(cls) -> ScoringMethod:\n",
    "        def exact_deviation_scoring(question: Question, prediction: float) -> float:\n",
    "            return abs(prediction - question.community_prediction) * -1\n",
    "\n",
    "        return ScoringMethod(score_function=exact_deviation_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def relative_entropy_score(cls) -> ScoringMethod:\n",
    "        def relative_entropy_scoring(question: Question, prediction: float) -> float:\n",
    "            p = prediction\n",
    "            q = question.community_prediction\n",
    "            return -(p * np.log2(p/q) + (1-p) * np.log2((1-p)/(1-q)))\n",
    "\n",
    "        return ScoringMethod(score_function=relative_entropy_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def ratio_score(cls) -> ScoringMethod:\n",
    "        def ratio_scoring(question: Question, prediction: float) -> float:\n",
    "            return -abs(prediction / question.community_prediction - 1)\n",
    "\n",
    "        return ScoringMethod(score_function=ratio_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def geometric_mean_score(cls) -> ScoringMethod:\n",
    "        def geometric_mean_scoring(question: Question, prediction: float) -> float:\n",
    "            return -abs(np.sqrt(prediction * (1-prediction)) -\n",
    "                       np.sqrt(question.community_prediction * (1-question.community_prediction)))\n",
    "\n",
    "        return ScoringMethod(score_function=geometric_mean_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def log_odds_ratio_score(cls) -> ScoringMethod:\n",
    "        def log_odds_ratio_scoring(question: Question, prediction: float) -> float:\n",
    "            p = prediction\n",
    "            c = question.community_prediction\n",
    "            log_odds_ratio = np.log((p / (1 - p)) / (c / (1 - c)))\n",
    "            return -abs(log_odds_ratio)\n",
    "\n",
    "        return ScoringMethod(score_function=log_odds_ratio_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def squared_log_odds_ratio_score(cls) -> ScoringMethod:\n",
    "        def squared_log_odds_ratio_scoring(question: Question, prediction: float) -> float:\n",
    "            p = prediction\n",
    "            c = question.community_prediction\n",
    "            log_odds_ratio = np.log((p / (1 - p)) / (c / (1 - c)))\n",
    "            return -(log_odds_ratio ** 2)\n",
    "\n",
    "        return ScoringMethod(score_function=squared_log_odds_ratio_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def expected_brier_score(cls) -> ScoringMethod:\n",
    "        def expected_brier_scoring(question: Question, prediction: float) -> float:\n",
    "            c = question.community_prediction\n",
    "            p = prediction\n",
    "            expected_score = c * (1 - p)**2 + (1 - c) * (1 - (1 - p))**2\n",
    "            return -expected_score\n",
    "\n",
    "        return ScoringMethod(score_function=expected_brier_scoring)\n",
    "\n",
    "    @classmethod\n",
    "    def expected_log_score(cls) -> ScoringMethod:\n",
    "        def expected_log_scoring(question: Question, prediction: float) -> float:\n",
    "            c = question.community_prediction\n",
    "            p = prediction\n",
    "            expected_score = c * np.log2(p) + (1 - c) * np.log2(1 - p)\n",
    "            return expected_score\n",
    "\n",
    "        return ScoringMethod(score_function=expected_log_scoring)\n",
    "\n",
    "\n",
    "class ForecasterCreator:\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_forecasters(cls) -> list[Forecaster]:\n",
    "        return [\n",
    "            cls.always_right_predictor(),\n",
    "            cls.always_wrong_predictor(),\n",
    "            *cls.get_all_non_omniscient_forecasters(),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_non_omniscient_forecasters(cls) -> list[Forecaster]:\n",
    "        return [\n",
    "            cls.random_predictor(),\n",
    "            cls.true_probability_predictor(),\n",
    "            cls.always_predict_50_percent(),\n",
    "            cls.randomly_below_50_percent(),\n",
    "            cls.randomly_above_50_percent(),\n",
    "            *cls.random_spread_predictors(10),\n",
    "            *cls.over_or_under_confidence_predictors(5, \"closer\"),\n",
    "            *cls.over_or_under_confidence_predictors(5, \"further\"),\n",
    "            *cls.fixed_bias_predictors(10),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def random_predictor(cls) -> Forecaster:\n",
    "        def randomly_predict(question: Question) -> float:\n",
    "            return rng.random()\n",
    "\n",
    "        return Forecaster(prediction_strategy=randomly_predict, name=\"Random Predictor\")\n",
    "\n",
    "    @classmethod\n",
    "    def true_probability_predictor(cls) -> Forecaster:\n",
    "        def predict_true_probability(question: Question) -> float:\n",
    "            return question.true_probability\n",
    "\n",
    "        return Forecaster(\n",
    "            prediction_strategy=predict_true_probability, name=\"True Probability\"\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def always_right_predictor(cls) -> Forecaster:\n",
    "        def predict_perfect(question: Question) -> float:\n",
    "            return 0.99 if question.resolution == True else 0.01\n",
    "\n",
    "        return Forecaster(prediction_strategy=predict_perfect, name=\"Always Right\")\n",
    "\n",
    "    @classmethod\n",
    "    def always_wrong_predictor(cls) -> Forecaster:\n",
    "        def predict_always_wrong(question: Question) -> float:\n",
    "            return 0.01 if question.resolution == True else 0.99\n",
    "\n",
    "        return Forecaster(prediction_strategy=predict_always_wrong, name=\"Always Wrong\")\n",
    "\n",
    "    @classmethod\n",
    "    def always_predict_50_percent(cls) -> Forecaster:\n",
    "        def predict_50_percent(question: Question) -> float:\n",
    "            return 0.5\n",
    "\n",
    "        return Forecaster(prediction_strategy=predict_50_percent, name=\"Always 50%\")\n",
    "\n",
    "    @classmethod\n",
    "    def randomly_below_50_percent(cls) -> Forecaster:\n",
    "        def predict_randomly_below_50_percent(question: Question) -> float:\n",
    "            return rng.uniform(0.01, 0.5)\n",
    "\n",
    "        return Forecaster(\n",
    "            prediction_strategy=predict_randomly_below_50_percent,\n",
    "            name=\"Randomly Below 50%\",\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def randomly_above_50_percent(cls) -> Forecaster:\n",
    "        def predict_randomly_above_50_percent(question: Question) -> float:\n",
    "            return rng.uniform(0.5, 0.99)\n",
    "\n",
    "        return Forecaster(\n",
    "            prediction_strategy=predict_randomly_above_50_percent,\n",
    "            name=\"Randomly Above 50%\",\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def random_spread_predictors(cls, num_predictors: int) -> list[Forecaster]:\n",
    "        skill_levels_spaced_out = np.linspace(0, 1, num_predictors, endpoint=False)\n",
    "\n",
    "        def predict_with_skill_level(question: Question, skill_level: float) -> float:\n",
    "            adjusted_skill_level = 1.5 - (\n",
    "                skill_level * 1.5\n",
    "            )  # Scale it to at max 1.5 and invert it. 1.5 chosen from manual experimentation. Lower value will lead to lower spread.\n",
    "            return cls.__clip_prediction(\n",
    "                rng.normal(question.true_probability, adjusted_skill_level)\n",
    "            )\n",
    "\n",
    "        forecasters = []\n",
    "        for skill_level in skill_levels_spaced_out:\n",
    "            forecasters.append(\n",
    "                Forecaster(\n",
    "                    prediction_strategy=lambda q, skill_level=skill_level: predict_with_skill_level(\n",
    "                        q, skill_level\n",
    "                    ),\n",
    "                    name=f\"Spread: Skill level {skill_level:.2f}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return forecasters\n",
    "\n",
    "    @classmethod\n",
    "    def over_or_under_confidence_predictors(\n",
    "        cls, num_predictors: int, bias_to_50_percent: Literal[\"closer\", \"further\"]\n",
    "    ) -> list[Forecaster]:\n",
    "        skill_levels = np.linspace(0, 1, num_predictors, endpoint=False)\n",
    "\n",
    "        def predict_with_random_bias(\n",
    "            question: Question,\n",
    "            skill_level: float,\n",
    "            bias_to_50_percent: Literal[\"closer\", \"further\"],\n",
    "        ) -> float:\n",
    "            base_prediction = question.true_probability\n",
    "            base_pred_above_50_percent = base_prediction > 0.5\n",
    "\n",
    "            if bias_to_50_percent == \"closer\":\n",
    "                max_allowed_delta = abs(base_prediction - 0.5)\n",
    "                add_or_subract_multiplier = -1 if base_pred_above_50_percent else 1\n",
    "                assert (\n",
    "                    abs(\n",
    "                        base_prediction\n",
    "                        + max_allowed_delta * add_or_subract_multiplier\n",
    "                        - 0.5\n",
    "                    )\n",
    "                    < 0.01\n",
    "                )\n",
    "            elif bias_to_50_percent == \"further\":\n",
    "                max_allowed_delta = 1 - abs(base_prediction - 0.5)\n",
    "                add_or_subract_multiplier = 1 if base_pred_above_50_percent else -1\n",
    "                zero_or_one_value = (\n",
    "                    base_prediction + max_allowed_delta * add_or_subract_multiplier\n",
    "                )\n",
    "                assert zero_or_one_value < 0.01 or zero_or_one_value > 0.99\n",
    "\n",
    "            scaled_max_delta = max_allowed_delta * (1 - skill_level)\n",
    "            random_delta_based_on_skill = (\n",
    "                rng.uniform(0, scaled_max_delta) * add_or_subract_multiplier\n",
    "            )\n",
    "            prediction = base_prediction + random_delta_based_on_skill\n",
    "            clipped_prediction = cls.__clip_prediction(prediction)\n",
    "            return clipped_prediction\n",
    "\n",
    "        forecasters = []\n",
    "        for i, skill_level in enumerate(skill_levels):\n",
    "            forecasters.append(\n",
    "                Forecaster(\n",
    "                    prediction_strategy=lambda q, skill_level=skill_level: predict_with_random_bias(\n",
    "                        q, skill_level, bias_to_50_percent\n",
    "                    ),\n",
    "                    name=f\"Confidence: Skill level {skill_level:.2f} (errors {bias_to_50_percent} to 50%)\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return forecasters\n",
    "\n",
    "    @classmethod\n",
    "    def fixed_bias_predictors(cls, num_predictors: int) -> list[Forecaster]:\n",
    "        bias_levels = np.linspace(-0.45, 0.5, num_predictors)\n",
    "\n",
    "        def predict_x_percentage_points_higher_than_true_probability(\n",
    "            question: Question, x: float\n",
    "        ) -> float:\n",
    "            return cls.__clip_prediction(question.true_probability + x)\n",
    "\n",
    "        forecasters = []\n",
    "        for bias in bias_levels:\n",
    "            forecasters.append(\n",
    "                Forecaster(\n",
    "                    prediction_strategy=lambda q, bias=bias: predict_x_percentage_points_higher_than_true_probability(\n",
    "                        q, bias\n",
    "                    ),\n",
    "                    name=f\"Fixed Bias: Deviate by {bias*100:.2f}% points\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return forecasters\n",
    "\n",
    "    @classmethod\n",
    "    def __clip_prediction(cls, prediction: float) -> float:\n",
    "        return np.clip(prediction, 0.01, 0.99)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "questions = QuestionManager.get_random_questions(10000)\n",
    "\n",
    "tournament = Tournament(\n",
    "    questions=questions,\n",
    "    forecasters=[\n",
    "        # *ForecasterCreator.get_all_forecasters()\n",
    "        *ForecasterCreator.get_all_non_omniscient_forecasters()\n",
    "    ],\n",
    "    scoring_methods=ScoringMethodCreator.get_all_scoring_methods(),\n",
    ")\n",
    "\n",
    "def visualize_tournament_results(tournament: Tournament) -> None:\n",
    "    num_scoring_methods = len(tournament.scoring_methods)\n",
    "    fig, axes = plt.subplots(num_scoring_methods, 1, figsize=(12, 6 * num_scoring_methods))\n",
    "    if num_scoring_methods == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, ranking in zip(axes, tournament.rankings):\n",
    "        scores = ranking.ranked_scores\n",
    "        names = [f.name for f in ranking.ranked_forecasters]\n",
    "\n",
    "        bars = sns.barplot(x=scores, y=names, ax=ax, palette='viridis')\n",
    "\n",
    "        if isinstance(bars, int):\n",
    "            raise ValueError(\"this is just to use the var 'bars' for sake of the ruff formatter\")\n",
    "\n",
    "        # Add value labels next to each bar\n",
    "        for i, score in enumerate(scores):\n",
    "            ax.text(score, i, f' {score:.2f}', va='center')\n",
    "\n",
    "        ax.set_title(f'Scores using {ranking.scoring_method.name}')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Forecaster')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_tournament_results(tournament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ranking_differences(tournament: Tournament) -> None:\n",
    "    # Get all scoring method names\n",
    "    scoring_methods = [r.scoring_method.name for r in tournament.rankings]\n",
    "    n_methods = len(scoring_methods)\n",
    "\n",
    "    # Create a matrix of ranking differences\n",
    "    diff_matrix = np.zeros((n_methods, n_methods))\n",
    "\n",
    "    # For each pair of scoring methods\n",
    "    for i, ranking1 in enumerate(tournament.rankings):\n",
    "        ranks1 = {forecaster.name: rank for rank, forecaster in enumerate(ranking1.ranked_forecasters)}\n",
    "\n",
    "        for j, ranking2 in enumerate(tournament.rankings):\n",
    "            ranks2 = {forecaster.name: rank for rank, forecaster in enumerate(ranking2.ranked_forecasters)}\n",
    "\n",
    "            # Calculate average absolute difference in rankings\n",
    "            rank_diffs = [abs(ranks1[name] - ranks2[name]) for name in ranks1.keys()]\n",
    "            diff_matrix[i, j] = np.mean(rank_diffs)\n",
    "\n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        diff_matrix,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        xticklabels=scoring_methods,\n",
    "        yticklabels=scoring_methods,\n",
    "        cmap='YlOrRd'\n",
    "    )\n",
    "    plt.title('Average Absolute Difference in Rankings Between Scoring Methods')\n",
    "    plt.xlabel('Scoring Method')\n",
    "    plt.ylabel('Scoring Method')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_ranking_differences(tournament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ranking_diffs(tournament: Tournament) -> None:\n",
    "    # Get baseline score rankings as reference\n",
    "    baseline_ranking = next(r for r in tournament.rankings if r.scoring_method.name == 'baseline_scoring')\n",
    "    baseline_ranks = {f.name: i for i, f in enumerate(baseline_ranking.ranked_forecasters)}\n",
    "\n",
    "    # Get ordered list of forecaster names from baseline (reversed to put best at top)\n",
    "    baseline_ordered_names = [f.name for f in baseline_ranking.ranked_forecasters][::-1]\n",
    "\n",
    "    # Create figure with a subplot for each non-baseline scoring method\n",
    "    non_baseline_rankings = [r for r in tournament.rankings if r.scoring_method.name != 'baseline_scoring']\n",
    "    fig, axes = plt.subplots(\n",
    "        len(non_baseline_rankings), 1,\n",
    "        figsize=(12, 8 * len(non_baseline_rankings)),\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    # For each scoring method\n",
    "    for idx, ranking in enumerate(non_baseline_rankings):\n",
    "        ax = axes[idx, 0]\n",
    "\n",
    "        # Calculate position changes\n",
    "        current_ranks = {f.name: i for i, f in enumerate(ranking.ranked_forecasters)}\n",
    "        position_changes = {\n",
    "            name: baseline_ranks[name] - current_ranks[name]\n",
    "            for name in baseline_ranks\n",
    "        }\n",
    "\n",
    "        # Create list of changes in baseline order\n",
    "        sorted_changes = [(name, position_changes[name]) for name in baseline_ordered_names]\n",
    "\n",
    "        # Create colors and positions for bars\n",
    "        colors = ['red' if change < 0 else 'green' for name, change in sorted_changes]\n",
    "        y_pos = np.arange(len(sorted_changes)) * 2.0\n",
    "\n",
    "        # Plot horizontal bars\n",
    "        bars = ax.barh(\n",
    "            y_pos,\n",
    "            [change for _, change in sorted_changes],\n",
    "            height=0.6,\n",
    "            color=colors,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "        # Add labels and formatting\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels([name for name, _ in sorted_changes], fontsize=8)\n",
    "        ax.set_title(\n",
    "            f'Ranking Changes: {ranking.scoring_method.name} vs Baseline Score\\n' +\n",
    "            '(green = ranked higher in this scoring method than baseline, red = ranked lower)',\n",
    "            pad=20, fontsize=12\n",
    "        )\n",
    "        ax.set_xlabel('Position Change', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            if width != 0:  # Only add label if there's a change\n",
    "                ax.text(\n",
    "                    width + np.sign(width) * 0.5,\n",
    "                    bar.get_y() + bar.get_height()/2,\n",
    "                    f'{int(abs(width))} ranks {\"higher\" if width > 0 else \"lower\"}',\n",
    "                    ha='left' if width > 0 else 'right',\n",
    "                    va='center',\n",
    "                    fontsize=8\n",
    "                )\n",
    "\n",
    "        # Adjust plot limits to show all labels\n",
    "        ax.set_ylim(min(y_pos) - 2, max(y_pos) + 2)\n",
    "\n",
    "    plt.tight_layout(pad=4.0)\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_ranking_diffs(tournament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################# TESTS ##############################################\n",
    "\n",
    "\n",
    "def test_brier_score_gives_correct_output() -> None:\n",
    "    question = Question(true_probability=0.7)\n",
    "    question.resolution = True\n",
    "    scoring = ScoringMethodCreator.brier_score()\n",
    "\n",
    "    perfect_prediction = 1.0\n",
    "    perfect_score = scoring.score_function(question, perfect_prediction)\n",
    "    assert abs(perfect_score) < 1e-10\n",
    "\n",
    "    worst_prediction = 0.0\n",
    "    worst_score = scoring.score_function(question, worst_prediction)\n",
    "    assert abs(worst_score + 1.0) < 1e-10\n",
    "\n",
    "\n",
    "def test_baseline_score_gives_correct_output() -> None:\n",
    "    scoring = ScoringMethodCreator.baseline_score()\n",
    "    test_cases = [\n",
    "        (0.70, 48, -74),\n",
    "        (0.80, 68, -132),\n",
    "        (0.90, 85, -232),\n",
    "        (0.99, 99, -564),\n",
    "    ] # https://www.metaculus.com/help/scores-faq/#:~:text=was%20right%3F%20below.-,Can%20I%20get%20better%20scores%20by%20predicting%20extreme%20values%3F,-Metaculus%20uses%20proper\n",
    "\n",
    "    for prediction, yes_score, no_score in test_cases:\n",
    "        question = Question(true_probability=prediction)\n",
    "\n",
    "        # Test when resolution is True\n",
    "        question.resolution = True\n",
    "        score_if_yes = scoring.score_function(question, prediction)\n",
    "        yes_diff = abs(score_if_yes - yes_score)\n",
    "        assert yes_diff < 1, f\"Failed for p={prediction}, yes case. Diff: {yes_diff}. Score: {score_if_yes}. Expected: {yes_score}\"\n",
    "\n",
    "        # Test when resolution is False\n",
    "        question.resolution = False\n",
    "        score_if_no = scoring.score_function(question, prediction)\n",
    "        no_diff = abs(score_if_no - no_score)\n",
    "        assert no_diff < 1, f\"Failed for p={prediction}, no case. Diff: {no_diff}. Score: {score_if_no}. Expected: {no_score}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_skill_level_predictors_in_right_order() -> None:\n",
    "    questions = QuestionManager.get_random_questions(10000)\n",
    "    predictors = ForecasterCreator.random_spread_predictors(5)\n",
    "\n",
    "    tournament = Tournament(\n",
    "        questions=questions,\n",
    "        forecasters=predictors,\n",
    "        scoring_methods=[ScoringMethodCreator.brier_score()],\n",
    "    )\n",
    "\n",
    "    initial_predictors_sorted_by_skill = sorted(predictors, key=lambda x: x.name, reverse=True)\n",
    "    final_ranked_predictors = tournament.rankings[0].ranked_forecasters\n",
    "\n",
    "    for initial_predictor, final_predictor in zip(initial_predictors_sorted_by_skill, final_ranked_predictors):\n",
    "        assert (\n",
    "            initial_predictor.name == final_predictor.name\n",
    "        ), (\n",
    "            f\"Predictor {initial_predictor.name} is not in the right order. Expected {final_predictor.name}. \"\n",
    "            f\"Final rankings: {[f.name for f in final_ranked_predictors]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def test_question_resolution_matches_true_probability() -> None:\n",
    "    num_trials = 10000\n",
    "    true_prob = 0.7\n",
    "    questions = [Question(true_probability=true_prob) for _ in range(num_trials)]\n",
    "    resolutions = [q.resolution for q in questions]\n",
    "    actual_prob = sum(resolutions) / num_trials\n",
    "    assert abs(actual_prob - true_prob) < 0.02, f\"Actual probability of resolution is {actual_prob}, expected {true_prob}\"\n",
    "\n",
    "\n",
    "def test_fixed_bias_predictors_maintain_correct_bias() -> None:\n",
    "    questions = QuestionManager.get_random_questions(1000)\n",
    "    biased_predictor = ForecasterCreator.fixed_bias_predictors(5)\n",
    "\n",
    "\n",
    "    for predictor in biased_predictor:\n",
    "        predictions = [predictor.predict(q) for q in questions]\n",
    "        true_probs = [q.true_probability for q in questions]\n",
    "\n",
    "        should_be_higher = predictions[0] - true_probs[0] > 0\n",
    "        for prediction, true_probability in zip(predictions, true_probs):\n",
    "            if should_be_higher:\n",
    "                assert prediction >= true_probability, (\n",
    "                    f\"Predictor {predictor.name} is not maintaining the correct bias. Expected higher prediction than true probability.\"\n",
    "                    f\"Prediction: {prediction}, True probability: {true_probability}\"\n",
    "                )\n",
    "            else:\n",
    "                assert prediction <= true_probability, (\n",
    "                    f\"Predictor {predictor.name} is not maintaining the correct bias. Expected lower prediction than true probability.\"\n",
    "                    f\"Prediction: {prediction}, True probability: {true_probability}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def test_over_confidence_predictors_move_away_from_50() -> None:\n",
    "    questions = [Question(true_probability=0.7) for _ in range(100)]\n",
    "    over_confident = ForecasterCreator.over_or_under_confidence_predictors(\n",
    "        1, \"further\"\n",
    "    )[0]\n",
    "\n",
    "    predictions = [over_confident.predict(q) for q in questions]\n",
    "    avg_prediction = sum(predictions) / len(predictions)\n",
    "\n",
    "    assert avg_prediction > 0.7\n",
    "\n",
    "\n",
    "def test_under_confidence_predictors_move_toward_50() -> None:\n",
    "    questions = [Question(true_probability=0.7) for _ in range(100)]\n",
    "    under_confident = ForecasterCreator.over_or_under_confidence_predictors(\n",
    "        1, \"closer\"\n",
    "    )[0]\n",
    "\n",
    "    predictions = [under_confident.predict(q) for q in questions]\n",
    "    avg_prediction = sum(predictions) / len(predictions)\n",
    "\n",
    "    assert avg_prediction < 0.7\n",
    "\n",
    "def test_predictors_score_in_obvious_order() -> None:\n",
    "    questions = QuestionManager.get_random_questions(1000)\n",
    "\n",
    "    tournament = Tournament(\n",
    "        questions=questions,\n",
    "        forecasters=[\n",
    "            ForecasterCreator.true_probability_predictor(),\n",
    "            ForecasterCreator.always_predict_50_percent(),\n",
    "            ForecasterCreator.randomly_above_50_percent(),\n",
    "        ],\n",
    "        scoring_methods=ScoringMethodCreator.get_all_scoring_methods(),\n",
    "    )\n",
    "\n",
    "    for ranking in tournament.rankings:\n",
    "        ranked_names = [f.name for f in ranking.ranked_forecasters]\n",
    "        assert ranked_names[0] == \"True Probability\"\n",
    "        assert ranked_names[1] == \"Always 50%\"\n",
    "        assert ranked_names[2] == \"Randomly Above 50%\"\n",
    "\n",
    "\n",
    "def test_all_question_sets_predictors_and_scoring_methods_get_same_ordering() -> None:\n",
    "    question_sets = QuestionManager.get_all_questions_sets(1000)\n",
    "    for question_set in question_sets:\n",
    "        tournament = Tournament(\n",
    "            questions=question_set,\n",
    "            forecasters=ForecasterCreator.get_all_non_omniscient_forecasters(),\n",
    "            scoring_methods=ScoringMethodCreator.get_all_scoring_methods(),\n",
    "        )\n",
    "\n",
    "        first_ranking_names = [f.name for f in tournament.rankings[0].ranked_forecasters]\n",
    "        for ranking in tournament.rankings[1:]:\n",
    "            current_ranking_names = [f.name for f in ranking.ranked_forecasters]\n",
    "            assert first_ranking_names == current_ranking_names, (\n",
    "                f\"Rankings differ between scoring methods for question set. \"\n",
    "                f\"First ranking: {first_ranking_names}, \"\n",
    "                f\"Current ranking ({ranking.scoring_method.name}): {current_ranking_names}\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brier_score_gives_correct_output()\n",
    "test_baseline_score_gives_correct_output()\n",
    "test_skill_level_predictors_in_right_order()\n",
    "test_question_resolution_matches_true_probability()\n",
    "test_fixed_bias_predictors_maintain_correct_bias()\n",
    "test_over_confidence_predictors_move_away_from_50()\n",
    "test_under_confidence_predictors_move_toward_50()\n",
    "test_predictors_score_in_obvious_order()\n",
    "# test_all_question_sets_predictors_and_scoring_methods_get_same_ordering()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
